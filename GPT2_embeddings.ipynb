{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "GPT2_Tokenize.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1C8QQNFgu4NpnLtxUS_yyuew5kRMZ46WH",
      "authorship_tag": "ABX9TyOyBbuylGKknkjlOpg/Dolv",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/NehaChaudhary311/GPT2-Language-Model/blob/master/GPT2_embeddings.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xj8L4QJdylKn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HYKO703oyxyM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!git clone https://github.com/huggingface/transformers.git"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WaotmxoQy0oL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cd transformers"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KqE4XL-2y2yQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pip install ."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OsARKl8tzB8J",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.cuda"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mmjHfzKSzEeM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Context-manager that changes the selected device.\n",
        "dev = torch.device(\"cuda : 0\" if torch.cuda.is_available() else \"cpu\")\n",
        "#Returns the number of GPUs available.\n",
        "n_gpu = torch.cuda.device_count()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PUmRkiLMzIF6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "import torch\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BLxT5pXdzTPn",
        "colab_type": "code",
        "outputId": "460f7e14-b13f-4b44-a7b5-bc82ba0369fe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 381
        }
      },
      "source": [
        "from transformers import GPT2Tokenizer\n",
        "from transformers import GPT2LMHeadModel\n",
        "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
        "model = GPT2LMHeadModel.from_pretrained('gpt2') \n",
        "model.eval()\n",
        "\n",
        "#Sentence taken from \n",
        "text = \"Hello, have an embedding day.\"\n",
        "#marked_text = \"[CLS] \" + text + \" [SEP]\"\n",
        "# Tokenize our sentence with the GPT2 tokenizer.\n",
        "tokenized_text = tokenizer.tokenize(text)\n",
        "print(\"Tokenized text : \" , tokenized_text)\n",
        "tokenized_text_rem = tokenized_text.copy()\n",
        "\n",
        "j = 0\n",
        "#Removal of splitted tokens\n",
        "for i in range(1, len(tokenized_text)):\n",
        "  if tokenized_text[i] in \",:;(){}\" :\n",
        "            i += 1\n",
        "  elif tokenized_text[i].startswith('Ġ'):\n",
        "    i += 1 \n",
        "  else :\n",
        "    break\n",
        "if i != len(tokenized_text):\n",
        "  for j in range(i, len(tokenized_text)):\n",
        "    if tokenized_text[j][0] in \"Ġ.!?,\":\n",
        "      break\n",
        "    else :\n",
        "      j += 1\n",
        "      \n",
        "else : \n",
        "  pass\n",
        "#tokenized_text_rem has the words after joining the splitted words      \n",
        "tokenized_text_rem[i-1:j] = [''.join(tokenized_text_rem[i-1:j])]\n",
        "print(\"Tokenized text after joining : \" ,tokenized_text_rem)\n",
        "\n",
        "#Encoding : maps words to key IDs in vocabulary ditionary\n",
        "#Encoding our already tokenized text with the GPT2 encode()\n",
        "text_index = tokenizer.encode(tokenized_text,add_prefix_space=True)\n",
        "print(\"Encoded(tokenized text) : \", text_index)\n",
        "\n",
        "#Finding\n",
        "embeddings = model.transformer.wte.weight[text_index,:]\n",
        "print(\"Word embedding : \", embeddings)\n",
        "print(embeddings.shape)\n",
        "\n",
        "\n",
        "\n",
        "#Taking average of to-be-removed-tokens only when 'i' is not equal to len(tokenized_text)\n",
        "#That means 'i' has gone over the list and didn't find any splitted word, which further implies\n",
        "#that we do no have to take average of the text_index\n",
        "\n",
        "avg = torch.mean(embeddings[i-1:j], dim=0, keepdim=True)\n",
        "embeddings = torch.cat([embeddings[:i-1], avg, embeddings[j:]], dim=0)\n",
        "print(embeddings)\n",
        "print(embeddings.shape)\n",
        "  \n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tokenized text :  ['Hello', ',', 'Ġhave', 'Ġan', 'Ġembed', 'ding', 'Ġday', '.']\n",
            "Tokenized text after joining :  ['Hello', ',', 'Ġhave', 'Ġan', 'Ġembedding', 'Ġday', '.']\n",
            "Encoded(tokenized text) :  [15496, 11, 423, 281, 11525, 12083, 1110, 13]\n",
            "Word embedding :  tensor([[-0.0687, -0.1327,  0.0112,  ...,  0.0715, -0.0297, -0.0477],\n",
            "        [ 0.0115, -0.0029,  0.0323,  ...,  0.0277, -0.0297, -0.0599],\n",
            "        [ 0.0760,  0.0788,  0.1640,  ...,  0.0574, -0.0805,  0.0066],\n",
            "        ...,\n",
            "        [-0.0110, -0.1773,  0.1143,  ...,  0.1397,  0.3021,  0.1670],\n",
            "        [-0.1379, -0.0294, -0.0026,  ..., -0.0966, -0.0726,  0.1160],\n",
            "        [ 0.0466, -0.0113,  0.0283,  ..., -0.0735,  0.0496,  0.0963]],\n",
            "       grad_fn=<IndexBackward>)\n",
            "torch.Size([8, 768])\n",
            "tensor([[-0.0687, -0.1327,  0.0112,  ...,  0.0715, -0.0297, -0.0477],\n",
            "        [ 0.0115, -0.0029,  0.0323,  ...,  0.0277, -0.0297, -0.0599],\n",
            "        [ 0.0760,  0.0788,  0.1640,  ...,  0.0574, -0.0805,  0.0066],\n",
            "        ...,\n",
            "        [-0.0504, -0.0939,  0.0657,  ...,  0.1176,  0.1851,  0.1278],\n",
            "        [-0.1379, -0.0294, -0.0026,  ..., -0.0966, -0.0726,  0.1160],\n",
            "        [ 0.0466, -0.0113,  0.0283,  ..., -0.0735,  0.0496,  0.0963]],\n",
            "       grad_fn=<CatBackward>)\n",
            "torch.Size([7, 768])\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}